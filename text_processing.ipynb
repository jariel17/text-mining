{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13fcd92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'funciones' from '/home/tux/text-mining/funciones.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import importlib\n",
    "import funciones\n",
    "importlib.reload(funciones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b612ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tux/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/tux/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/tux/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tux/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tux/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/tux/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path.append('/home/tux/nltk_data')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92998306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# url = 'https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv'\n",
    "df = pd.read_csv('data/sms.tsv', sep='\\t', names=['label', 'message'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de576b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [go, jurong, point, crazy, available, bugis, n...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
      "3        [u, dun, say, early, hor, u, c, already, say]\n",
      "4    [nah, dont, think, goes, usf, lives, around, t...\n",
      "Name: cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['cleaned'] = funciones.preprocess(df['message'])\n",
    "print(df['cleaned'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "230c2cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [go, jurong, point, crazy, available, bugis, n...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
      "3        [u, dun, say, early, hor, u, c, already, say]\n",
      "4    [nah, dont, think, go, usf, life, around, though]\n",
      "Name: lemmas, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['lemmas'] = funciones.lemmatize(df['cleaned'])\n",
    "print(df['lemmas'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ca2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message  \\\n",
      "0      ham  Go until jurong point, crazy.. Available only ...   \n",
      "1      ham                      Ok lar... Joking wif u oni...   \n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3      ham  U dun say so early hor... U c already then say...   \n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "...    ...                                                ...   \n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
      "5568   ham               Will ü b going to esplanade fr home?   \n",
      "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
      "5570   ham  The guy did some bitching but I acted like i'd...   \n",
      "5571   ham                         Rofl. Its true to its name   \n",
      "\n",
      "                                                cleaned  \\\n",
      "0     [go, jurong, point, crazy, available, bugis, n...   \n",
      "1                        [ok, lar, joking, wif, u, oni]   \n",
      "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
      "3         [u, dun, say, early, hor, u, c, already, say]   \n",
      "4     [nah, dont, think, goes, usf, lives, around, t...   \n",
      "...                                                 ...   \n",
      "5567  [2nd, time, tried, 2, contact, u, u, £750, pou...   \n",
      "5568                 [ü, b, going, esplanade, fr, home]   \n",
      "5569                   [pity, mood, soany, suggestions]   \n",
      "5570  [guy, bitching, acted, like, id, interested, b...   \n",
      "5571                                 [rofl, true, name]   \n",
      "\n",
      "                                                 lemmas  \n",
      "0     [go, jurong, point, crazy, available, bugis, n...  \n",
      "1                        [ok, lar, joking, wif, u, oni]  \n",
      "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
      "3         [u, dun, say, early, hor, u, c, already, say]  \n",
      "4     [nah, dont, think, go, usf, life, around, though]  \n",
      "...                                                 ...  \n",
      "5567  [2nd, time, try, 2, contact, u, u, £750, pound...  \n",
      "5568                    [ü, b, go, esplanade, fr, home]  \n",
      "5569                    [pity, mood, soany, suggestion]  \n",
      "5570  [guy, bitch, act, like, id, interested, buying...  \n",
      "5571                                 [rofl, true, name]  \n",
      "\n",
      "[5572 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "## Visualizar las 20  palabras más frecuentes y por clase\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
